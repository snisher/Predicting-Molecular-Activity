{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PMB.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHSLI-S31cf-",
        "colab_type": "text"
      },
      "source": [
        "#Predicting Molecular Bioactivity\n",
        "\n",
        "Fisher Moritzburke (fmoritzb) & \n",
        "Martin Hoffmann (maedhoff)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3Y9tk7V1Y-H",
        "colab_type": "code",
        "outputId": "255c58a5-d245-4140-96d4-ebbcc4ed77e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import sklearn as sk\n",
        "from sklearn.model_selection import train_test_split # split into training/test\n",
        "from sklearn import preprocessing  # for standardizing the data\n",
        "from sklearn import metrics  # Useful for creating confusion matrices\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import backend as K\n",
        "Activation = keras.layers.Activation\n",
        "to_categorical = keras.utils.to_categorical\n",
        "Sequential = keras.Sequential\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from keras.layers.merge import add\n",
        "\n",
        "import math\n",
        "import time\n",
        "\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "import warnings  \n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvIfRaF9116d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datasets = [\n",
        "    ('HIVINT', '18wUy7Ax3LcpbOWEWxnd6vZ2q7K6Bd9IA'), #15.3MB\n",
        "    ('HIVPROT', '1kQZWm-Nxf5M-do_LwNabPEfMpdVKmLxZ'), #37.1MB\n",
        "    #('TDI', '1QAM7yM4pnN8_cLsTkccMhZZxFr8f9xRQ'), #47.8MB\n",
        "    ('OX1', '15bNN3_eyKlgGLOqN-veLgNLRqFAdU64P'), #49.5MB\n",
        "    ('THROMBIN', '11RJO5lG-pd1a_GrtraXx811wfkJHG-16'), #53.6MB\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qICV0CCv1-sX",
        "colab_type": "text"
      },
      "source": [
        "#Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2fHAdho2Ckb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# takes a (int or float) and returns a boolean\n",
        "def greater_than_zero(val):\n",
        "  # returns True if the value is greater than 0\n",
        "  assert isinstance(val, (int, float))\n",
        "  return(True if val>0 else False)\n",
        "\n",
        "# takes a pandas dataframe and an int, returns a pandas dataframe\n",
        "def drop_irrelevant_columns(data, n):\n",
        "  # drops columns from the dataframe with less than n non-zero values\n",
        "  num_columns = len(data.columns)\n",
        "  to_drop = []\n",
        "  for col in data.columns[2:]:\n",
        "    if list(map(greater_than_zero, data[col])).count(True) < n:\n",
        "      to_drop.append(col)\n",
        "\n",
        "  data = data.drop(columns=to_drop)\n",
        "  \n",
        "  remaining_percentage = round( ((len(data.columns)-2)/num_columns)*100, 2)\n",
        "  print(f'{len(to_drop)} columns have been dropped. {len(data.columns)} columns ({remaining_percentage}%) remain.')\n",
        "  \n",
        "  return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9gDdEDuF5GxA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# takes a (int or float) and returns a (int or float)\n",
        "def log(val):\n",
        "  if not isinstance(val, (int, float)):\n",
        "    return val\n",
        "  if val == 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return math.log(val+1, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rfhS43JO2FdB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# r_square accuracy metric function\n",
        "def r_square(y_true, y_pred):\n",
        "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
        "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
        "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Fn6-xsY2J7p",
        "colab_type": "text"
      },
      "source": [
        "#Build and evaluate models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgk_g9Dy2H4t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run(dataset_name, dataset_file_id, min_molecules_per_column = -1,\n",
        "           log_transform=True, \n",
        "           nn_epochs=150,\n",
        "           autoencoder_epochs=50,\n",
        "           nn_ae_epochs=150,\n",
        "           resMLP_epochs=200,\n",
        "           verbose=2):\n",
        "  print(f\"{dataset_name} data set...\")\n",
        "  \n",
        "  histories = {}\n",
        "  \n",
        "  Dropout = keras.layers.Dropout\n",
        "  Dense = keras.layers.Dense\n",
        "  \n",
        "  # 1) fetch data from google drive\n",
        "  link = 'https://drive.google.com/uc?export=download&id={FILE_ID}'\n",
        "  csv_url = link.format(FILE_ID = dataset_file_id)\n",
        "  try:\n",
        "    data = pd.read_csv(csv_url, sep=',')\n",
        "  except Exception as e:\n",
        "    print('There was an error parsing the csv file!')\n",
        "    return\n",
        "  \n",
        "  # 2) remove irrelevant columns:\n",
        "  if min_molecules_per_column > -1:\n",
        "    data = drop_irrelevant_columns(data, min_molecules_per_column)\n",
        "    \n",
        "  # 3) apply a logarithmic transformation:\n",
        "  if log_transform:\n",
        "    data = data.applymap(log)\n",
        "  \n",
        "  # 4) split the dataset in training and test data:\n",
        "  cols = data.columns.tolist()[2:]    # exclude MOLECULE and Act\n",
        "  X = data[cols]\n",
        "  y = data['Act']\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=0)\n",
        "  \n",
        "  # make sure data is numpy array\n",
        "  if isinstance(X_train, pd.DataFrame):\n",
        "    X_train = X_train.values\n",
        "    X_test = X_test.values\n",
        "  \n",
        "  # 5) build, train, & evaluate the models\n",
        "  \n",
        "  #============ Standard MLP ===============#\n",
        "  \n",
        "  model = Sequential()\n",
        "\n",
        "  input_dim = X_train.shape[1]\n",
        "\n",
        "  # Add layers\n",
        "  model.add( Dense(2000, activation='relu', input_dim = input_dim))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Dense(1000, activation='relu'))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Dense(1000, activation='relu'))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Dense(1000, activation='relu'))\n",
        "  model.add(Dropout(0.1))\n",
        "  # Layer 5, output layer\n",
        "  model.add(Dense(1, activation = 'linear'))\n",
        "\n",
        "  # Compile the NN model, defining the optimizer to use, the loss function, and the metrics to use.\n",
        "  model.compile(optimizer = 'RMSprop',\n",
        "               loss = 'mse',\n",
        "               metrics = [r_square])\n",
        "  \n",
        "  # fit the model to the training data:\n",
        "  print('training the MLP...')\n",
        "  history = model.fit(X_train, \n",
        "    y_train,\n",
        "    validation_split = 0.2, \n",
        "    epochs = nn_epochs, \n",
        "    batch_size = 50,\n",
        "    verbose = verbose,\n",
        "  )\n",
        "  \n",
        "  # Evaluate the MLP's performance\n",
        "  evaluation = {}\n",
        "  \n",
        "  train_loss, train_acc = model.evaluate(X_train, y_train) # evaluate\n",
        "  test_loss, test_acc = model.evaluate(X_test, y_test) # evaluate\n",
        "  \n",
        "  evaluation['train_acc'] = train_acc\n",
        "  evaluation['test_acc'] = test_acc\n",
        "  evaluation['train_loss'] = train_loss\n",
        "  evaluation['test_loss'] = test_loss\n",
        "  \n",
        "  print('MLP test set accuracy:', test_acc, '\\n')\n",
        "  \n",
        "  histories['MLP'] = {'history': history.history,\n",
        "                      'evaluation' : evaluation}\n",
        "  \n",
        "  \n",
        "  #============ Encoder + MLP ===============#\n",
        "  \n",
        "  # create the autoencoder:\n",
        "  input_dim = X_train.shape[1] # the input size\n",
        "  encoding_dim = 50 # the size of the encoded representation\n",
        "  compression_factor = float(input_dim) / encoding_dim\n",
        "\n",
        "  autoencoder = Sequential() # create sequential model\n",
        "  # Encoder Layers\n",
        "  autoencoder.add(Dense(input_dim/4, input_shape=(input_dim,), activation='relu'))\n",
        "  autoencoder.add(Dense(input_dim/10, activation='relu'))\n",
        "  autoencoder.add(Dense(encoding_dim, activation='relu')) # encoded state\n",
        "  # Decoding Layers\n",
        "  autoencoder.add(Dense(input_dim/10, activation='relu'))\n",
        "  autoencoder.add(Dense(input_dim/4, activation='relu'))\n",
        "  autoencoder.add(Dense(input_dim, activation='relu'))\n",
        "\n",
        "  autoencoder.compile(optimizer='RMSprop',\n",
        "    loss = 'mse', metrics=[r_square])\n",
        "\n",
        "  autoencoder_history = autoencoder.fit(X_train, X_train,\n",
        "    epochs=autoencoder_epochs, #50\n",
        "    batch_size=256,\n",
        "    validation_data=(X_test, X_test),\n",
        "    verbose = verbose,\n",
        "  )\n",
        "  \n",
        "  \n",
        "  # build a new model using the autoencoder as a basis:\n",
        "  ae_model = Sequential()\n",
        "\n",
        "  # Fist add the encoding layers trained previously\n",
        "  ae_model.add(autoencoder.layers[0])\n",
        "  ae_model.add(autoencoder.layers[1])\n",
        "  ae_model.add(autoencoder.layers[2])\n",
        "\n",
        "  # add an additional 2 FC layers\n",
        "  ae_model.add(Dense(50, activation='relu'))\n",
        "  ae_model.add(Dropout(0.25))\n",
        "  ae_model.add(Dense(20, activation='relu'))\n",
        "  ae_model.add(Dropout(0.1))\n",
        "  # output layer\n",
        "  ae_model.add(Dense(1, activation = 'linear'))\n",
        "  \n",
        "  ae_model.compile(optimizer = 'RMSprop',\n",
        "             loss = 'mse',\n",
        "             metrics = [r_square])\n",
        "\n",
        "  print('Training the encoder + MLP...')\n",
        "  \n",
        "  history = ae_model.fit(X_train, \n",
        "    y_train,\n",
        "    validation_split = 0.33, \n",
        "    epochs = nn_ae_epochs, #300 \n",
        "    batch_size = 32,\n",
        "    verbose = verbose,\n",
        "  )\n",
        "  \n",
        "  # Evaluate the model's performance\n",
        "  evaluation = {}\n",
        "  \n",
        "  train_loss, train_acc = ae_model.evaluate(X_train, y_train) # evaluate\n",
        "  test_loss, test_acc = ae_model.evaluate(X_test, y_test) # evaluate\n",
        "  \n",
        "  evaluation['train_acc'] = train_acc\n",
        "  evaluation['test_acc'] = test_acc\n",
        "  evaluation['train_loss'] = train_loss\n",
        "  evaluation['test_loss'] = test_loss\n",
        "\n",
        "  print('encoder + MLP test set accuracy:', test_acc, '\\n')\n",
        "  \n",
        "  histories['ae_MLP'] = {'history': history.history, \n",
        "                         'evaluation': evaluation}\n",
        "  \n",
        "  #============ residual MLP ===============# \n",
        "  from keras.layers import Input, Dense, Dropout\n",
        "  \n",
        "  # residualBlock layer. Returns the outputs and the new size of the tensor\n",
        "  def residualBlock(inputs, size):\n",
        "    outputs = Dense(size, activation='relu')(inputs) # Dense relu\n",
        "    outputs = Dropout(.2)(outputs) # dropout\n",
        "    outputs = add([outputs, inputs]) # add layers\n",
        "    outputs = Dense(size//2, activation='relu')(outputs)\n",
        "    return outputs, size//2\n",
        "  \n",
        "  inSize = X_train.shape[1] # size of input\n",
        "  \n",
        "  visible = Input(shape=(inSize,), name='input') # input layer\n",
        "  x, size = residualBlock(visible, inSize)\n",
        "  x, size = residualBlock(x, size)\n",
        "  x, size = residualBlock(x, size)\n",
        "  x, size = residualBlock(x, size)\n",
        "  x, size = residualBlock(x, size)\n",
        "  x, size = residualBlock(x, size)\n",
        "  x, size = residualBlock(x, size)\n",
        "  x, size = residualBlock(x, size)\n",
        "  output = Dense(1, activation='linear')(x)\n",
        "  \n",
        "  resMLP = Model(inputs=visible, outputs=output)\n",
        "  \n",
        "  resMLP.compile(optimizer = 'RMSprop',\n",
        "                loss = 'mse',\n",
        "                metrics = [r_square])\n",
        "  \n",
        "  print('Training the resMLP...')\n",
        "  \n",
        "  history = resMLP.fit(X_train, \n",
        "                      y_train,\n",
        "                      validation_split = 0.2, \n",
        "                      epochs = resMLP_epochs,\n",
        "                      batch_size = 100,\n",
        "                      verbose = verbose)\n",
        "  \n",
        "  # Evaluate the model's performance\n",
        "  evaluation = {}\n",
        "  train_loss, train_acc = resMLP.evaluate(X_train, y_train) # evaluate train\n",
        "  test_loss, test_acc = resMLP.evaluate(X_test, y_test) # evaluate test\n",
        "  # save evaluations\n",
        "  evaluation['train_acc'] = train_acc\n",
        "  evaluation['test_acc'] = test_acc\n",
        "  evaluation['train_loss'] = train_loss\n",
        "  evaluation['test_loss'] = test_loss\n",
        "  \n",
        "  print('residual MLP test set accuracy:', test_acc)\n",
        "  \n",
        "  histories['resMLP'] = {'history': history.history,\n",
        "                         'evaluation': evaluation}\n",
        "\n",
        "  print('\\n')\n",
        "  \n",
        "  return histories"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7PXg_waL2N1s",
        "colab_type": "code",
        "outputId": "5c67a51d-89e1-45f1-ba61-3e1265e0f5b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print('Building models')\n",
        "results = {}\n",
        "\n",
        "for dataset in datasets:\n",
        "  name = dataset[0]\n",
        "\n",
        "  results[name] = run(\n",
        "    dataset_name             = name, \n",
        "    dataset_file_id          = dataset[1],\n",
        "    min_molecules_per_column = 30,\n",
        "    log_transform            = True,\n",
        "    nn_epochs=150, \n",
        "    autoencoder_epochs=50,\n",
        "    nn_ae_epochs=200,\n",
        "    resMLP_epochs=200, \n",
        "    verbose=0\n",
        "  )"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building models\n",
            "HIVINT data set...\n",
            "2388 columns have been dropped. 1800 columns (42.93%) remain.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "training MLP...\n",
            "1452/1452 [==============================] - 0s 56us/sample - loss: 0.0016 - r_square: -0.5941\n",
            "363/363 [==============================] - 0s 61us/sample - loss: 0.0018 - r_square: -0.5738\n",
            "MLP test set accuracy: -0.57381445 \n",
            "\n",
            "Training the encoder + MLP model...\n",
            "1452/1452 [==============================] - 0s 59us/sample - loss: 2.6541e-04 - r_square: 0.7492\n",
            "363/363 [==============================] - 0s 61us/sample - loss: 4.7724e-04 - r_square: 0.5710\n",
            "encoder + MLP test set accuracy: 0.5710208 \n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "Training the resMLP model...\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "1452/1452 [==============================] - 0s 65us/step\n",
            "363/363 [==============================] - 0s 77us/step\n",
            "residual MLP test set accuracy: 0.5145937744579367\n",
            "\n",
            "\n",
            "HIVPROT data set...\n",
            "3572 columns have been dropped. 2181 columns (37.88%) remain.\n",
            "training MLP...\n",
            "2569/2569 [==============================] - 0s 64us/sample - loss: 0.0033 - r_square: 0.6008\n",
            "643/643 [==============================] - 0s 64us/sample - loss: 0.0072 - r_square: 0.4071\n",
            "MLP test set accuracy: 0.40708205 \n",
            "\n",
            "Training the encoder + MLP model...\n",
            "2569/2569 [==============================] - 0s 62us/sample - loss: 0.0013 - r_square: 0.8455\n",
            "643/643 [==============================] - 0s 60us/sample - loss: 0.0055 - r_square: 0.6239\n",
            "encoder + MLP test set accuracy: 0.6239351 \n",
            "\n",
            "Training the resMLP model...\n",
            "2569/2569 [==============================] - 0s 69us/step\n",
            "643/643 [==============================] - 0s 68us/step\n",
            "residual MLP test set accuracy: 0.5259929030531116\n",
            "\n",
            "\n",
            "OX1 data set...\n",
            "2788 columns have been dropped. 1815 columns (39.39%) remain.\n",
            "training MLP...\n",
            "4280/4280 [==============================] - 0s 53us/sample - loss: 6.6432e-04 - r_square: 0.8669\n",
            "1071/1071 [==============================] - 0s 58us/sample - loss: 0.0014 - r_square: 0.7237\n",
            "MLP test set accuracy: 0.72372043 \n",
            "\n",
            "Training the encoder + MLP model...\n",
            "4280/4280 [==============================] - 0s 58us/sample - loss: 5.9295e-04 - r_square: 0.8782\n",
            "1071/1071 [==============================] - 0s 62us/sample - loss: 0.0013 - r_square: 0.7366\n",
            "encoder + MLP test set accuracy: 0.7365598 \n",
            "\n",
            "Training the resMLP model...\n",
            "4280/4280 [==============================] - 0s 65us/step\n",
            "1071/1071 [==============================] - 0s 67us/step\n",
            "residual MLP test set accuracy: 0.6860238092731472\n",
            "\n",
            "\n",
            "THROMBIN data set...\n",
            "3 columns have been dropped. 2 columns (0.0%) remain.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2656\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2657\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Act'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3bbf5408a919>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mnn_ae_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mresMLP_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m   )\n",
            "\u001b[0;32m<ipython-input-6-8949e279aa0a>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(dataset_name, dataset_file_id, min_molecules_per_column, log_transform, nn_epochs, autoencoder_epochs, nn_ae_epochs, resMLP_epochs, verbose)\u001b[0m\n\u001b[1;32m     33\u001b[0m   \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m    \u001b[0;31m# exclude MOLECULE and Act\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m   \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m   \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Act'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m   \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2925\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2926\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2927\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2657\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2658\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2659\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Act'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO72P4en2ivx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print evaluation for each dataset\n",
        "for dataset in results.keys():\n",
        "  print('Dataset:', dataset)\n",
        "  print('  MLP - test accuracy', '=', results[dataset]['MLP']['evaluation']['test_acc'])\n",
        "  print('  Encoder+MLP - test accuracy', '=', results[dataset]['ae_MLP']['evaluation']['test_acc'])\n",
        "  print('  Residual MLP - test accuracy', '=', results[dataset]['resMLP']['evaluation']['test_acc'])\n",
        "  print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSp6deeV6zCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}